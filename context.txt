================================================================================
MONSTER CHESS NN - CURRENT CONTEXT (COMPACT)
================================================================================
Last updated: 2026-02-19
Purpose: Fast technical handoff context aligned with current code state.

1) PROJECT
================================================================================
This project trains a neural network to play Monster Chess using a self-play loop
with MCTS, data processing, model training, and candidate-vs-incumbent gating.

Monster Chess rules in this project:
- White: King + 4 pawns, takes TWO consecutive moves per turn.
- Black: standard full army, takes ONE move per turn.
- King capture ends game (no checkmate semantics).
- White cannot castle.

Starting FEN:
rnbqkbnr/pppppppp/8/8/8/8/2PPPP2/4K3 w kq - 0 1

2) CORE PIPELINE
================================================================================
Loop in `src/iterate.py`:
1. Generate data via `src/data_generation.py`
2. Process JSONL games via `src/data_processor.py`
3. Train candidate via `src/train.py`
4. Gate candidate vs incumbent via arena matches
5. Promote only if gate passes

3) WHAT IS IMPLEMENTED (IMPORTANT)
================================================================================
Implemented in current code:
- Game-level split integrity (prevents train/val/test leakage by game)
- Deterministic seed controls and metadata logging in train/iterate runs
- True MSE + MAE reporting alongside value power loss
- Arena gating with color-swap and side-aware acceptance thresholds
- Alternating-side training with frozen/pool opponents
- AdamW with no-decay param groups for biases/norm layers
- Gradient clipping
- Warmup fine-tuning from incumbent model
- Baseline snapshot utility (`src/baseline_snapshot.py`)
- Human game evaluation utility (`src/human_eval.py`)

4) KNOWN OPEN GAPS
================================================================================
Not yet implemented or still partial:
- Position-budget windowing min-budget works (`--position-budget`);
  optional max-cap behavior is not yet implemented
- WDL value head (pending)
- Separate White/Black networks (pending)

5) MOST IMPORTANT BEHAVIORAL ISSUE
================================================================================
Asymmetry remains the dominant challenge:
- White-side performance improves faster than Black-side performance.
- Side-aware gate can reject candidates that fail the non-trained side floor.

6) KEY FILE MAP
================================================================================
src/config.py             Hyperparameters, paths, curriculum FENs
src/monster_chess.py      Game rules wrapper
src/mcts.py               UCB + PUCT search
src/evaluation.py         Heuristic + NN evaluator wrapper
src/data_generation.py    Self-play generation
src/data_processor.py     Game JSONL -> numpy tensors
src/train.py              Dual-head residual model training
src/iterate.py            Iteration loop + gating + metadata
src/baseline_snapshot.py  Baseline artifact capture
src/human_eval.py         Human game diagnostics
src/play.py               CLI human vs AI

7) VERIFIED LOCAL SNAPSHOT (2026-02-19)
================================================================================
- Highest raw generation directory observed: nn_gen24
- Processed positions shape: (86990, 8, 8, 15)
- Processed splits: train=69592, val=8699, test=8699
- Human game files: 25
- Current model: models/best_value_net.pt
- Current model SHA256:
  D54E3117A7A28BD4C0C678FE2D3FAB231AA8943660CFBC1FF6958E9B70BC815B

8) DOCUMENTS TO TRUST FIRST
================================================================================
- README.md                         onboarding + command map
- IMPROVEMENT_PLAN.md              living status of roadmap items
- IMPROVEMENT_EXECUTION_ORDER.md   next implementation sequence
- TRANSFER_HANDOFF_2026-02-19.md   historical + current transfer notes

9) QUICK OPERATING COMMANDS
================================================================================
Baseline snapshot:
  py -3 src/baseline_snapshot.py

Iterative training (example):
  py -3 src/iterate.py --iterations 2 --games 180 --curriculum-games 220 \
    --black-focus-games 260 --simulations 120 --curriculum-simulations 50 \
    --black-focus-simulations 100 --epochs 12 --warmup-epochs 3 \
    --warmup-start-factor 0.1 --keep-generations 3 --alternating \
    --opponent-sims 140 --pool-size 6 --arena-games 80 --arena-sims 80 \
    --arena-workers 4 --gate-threshold 0.54 --gate-min-other-side 0.42 \
    --seed 20260219 --human-eval

Human eval:
  py -3 src/human_eval.py --human-dir data/raw/human_games --model models/best_value_net.pt

================================================================================
END CONTEXT
================================================================================